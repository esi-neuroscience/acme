Advanced Usage and Customization
================================
One of ACME's main objectives is to be as application-agnostic as possible
so that it can be a drop-in parallelization engine for a wide spectrum of
use cases. Thus, ACME features a variety of customization options for
custom-tailoring its functionality to a specific problem. This section
summarizes some technical intricacies and settings details.

.. contents:: Quick Links
    :depth: 3

User-Function Requirements
--------------------------
The user-provided function `func` has to meet some basic requirements to
permit parallel execution with :class:`~acme.ParallelMap`:

* **input arguments of `func`** should be regular Python objects (lists, tuples,
  scalars, strings etc.) or NumPy arrays. Custom user-defined classes
  may or may not work. In general, anything that can be serialized via
  `cloudpickle <https://pypi.org/project/cloudpickle/>`_ should work out of the box.

* if automatic result saving is used (`write_worker_results` is `True`),
  the **return value(s) of `func`** have to be suitable for storage in HDF5
  containers. Thus, anything returned by `func` should be either purely
  numeric (scalars or NumPy arrays) or purely lexical (strings). Hybrid
  text/numeric data-types (e.g., Pandas dataframes), custom class instances,
  functions, generators or complex objects (like matplotlib figures)
  **will not work**.

Auto-Generated HDF5-Files
-------------------------
All HDF5 files auto-generated by :class:`~acme.ParallelMap` are stored in a directory
*ACME_YYYYMMDD-hhmmss-ffffff* (encoding the current time as
*YearMonthDay-HourMinuteSecond-Microsecond*) that is created in the user's
home directory on ``/cs`` (if ACME is running on the ESI HPC cluster) or the
current working directory (if running locally).

The HDF5 files themselves
are named *funcname_workerid.h5*, where `funcname` is the name of the user-provided
function and `workerid` encodes the number of the worker that generated
the file (see `Walkthrough`_ above for examples).
The internal structure of all HDF5 files is kept as simple as possible:
each return value of the user-provided function `func` is saved in a
separate dataset in the file's root group. For instance, processing
the following user-provided function

.. code-block:: python

    def this_func(a, b, c):
        # ...some complicated calculations...
        return r0, r1, r2

with 50 workers using ``write_worker_results = True`` yields 50 HDF5
files *this_func_0.h5*, *this_func_1.h5*, ..., *this_func_49.h5* each
containing three datasets `"result_0"` (holding `r0`), `"result_1"`
(holding `r1`) and `"result_2"` (holding `r2`). User-provided functions
with only a single return value correspondingly yield HDF5 files that
only contain one dataset (`"result_0"`) in their respective root group.



