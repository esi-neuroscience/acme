Advanced Usage and Customization
================================
One of ACME's main objectives is to be as application-agnostic as possible
so that it can be a drop-in parallelization engine for a wide spectrum of
use cases. Thus, ACME features a variety of customization options for
custom-tailoring its functionality to a specific problem. This section
summarizes some technical intricacies and settings details.

.. contents:: Quick Links
    :depth: 3

User-Function Requirements
--------------------------
The user-provided function `func` has to meet some basic requirements to
permit parallel execution with :class:`~acme.ParallelMap`:

* **input arguments of `func`** should be regular Python objects (lists, tuples,
  scalars, strings etc.) or NumPy arrays. Custom user-defined classes
  may or may not work. In general, anything that can be serialized via
  `cloudpickle <https://pypi.org/project/cloudpickle/>`_ should work out of the box.

* if automatic result saving is used (`write_worker_results` is `True`),
  the **return value(s) of `func`** have to be suitable for storage in HDF5
  containers. Thus, anything returned by `func` should be either purely
  numeric (scalars or NumPy arrays) or purely lexical (strings). Hybrid
  text/numeric data-types (e.g., Pandas dataframes), custom class instances,
  functions, generators or complex objects (like matplotlib figures)
  **will not work**.

.. _hdf5files:

Auto-Generated HDF5-Files
-------------------------
Directory Layout
^^^^^^^^^^^^^^^^
All HDF5 files auto-generated by :class:`~acme.ParallelMap` are stored in a directory
named ``ACME_YYYYMMDD-hhmmss-ffffff`` (encoding the current time as
``YearMonthDay-HourMinuteSecond-Microsecond``) that is created in the user's
home directory on ``/cs`` (if ACME is running on the ESI HPC cluster) or the
current working directory (if running locally).

By default, ACME generates ``n_inputs + 1`` HDF5 files: one aggregate results
container and ``n_inputs`` HDF5 files for each computation. The aggregate
"main file" really only contains links to the payload files and is called
``funcname.h5`` based on the name of the user-provided function.
The naming of the payload files follows the schema ``funcname_compid.h5``,
where ``compid`` encodes the number of the performed computation (between 0
and ``n_inputs``):

::

    ACME_YYYYMMDD-hhmmss-ffffff/
      └─ funcname.h5
      └─ funcname_payload/
           └─ funcname_0.h5
           └─ funcname_1.h5
           └─ funcname_2.h5
           .
           .
           .
           └─ funcname_{n_inputs - 1}.h5

The main file ``funcname.h5`` is composed of relative links to the payload files in
the ``funcname_payload`` subdirectory. This means, ``funcname.h5`` is only
functional alongside its "payload" sub-folder. Therefore, if ``funcname.h5``
is moved to another file-system location, its payload  directory needs to be
moved as well.

.. note::

    The number of computations carried out (i.e., how many times a user-provided
    function ``func`` is called) does **not** have to be equals to the number of parallel
    workers used to perform these calculations. Particularly if ``n_inputs``
    is large but evaluating ``func`` is computationally cheap, starting
    ``n_inputs`` SLURM workers (=jobs) might take longer than the actual concurrent
    calculation. In such a scenario it is usually better to use fewer workers
    (set ``n_workers < n_inputs``) that each perform multiple evaluations of ``func``.

To create a container with the payload not distributed across external files
but included as regular datasets, h5py's ``copy`` routine can be used:

.. code-block:: python

    import h5py

    with h5py.File("new_file.h5", "w") as h5target:
        with h5py.File(pmap.results_container, "r")  as h5source:
            for comp in h5source.keys():
                h5source.copy(comp, h5target, expand_external=True, name=comp)

Note that ACME itself can create an aggregate results container with "regular" datasets
by setting the keyword ``single_file`` to ``True`` in :class:`~acme.ParallelMap`.
The potential benefits and downsides are discussed in :ref:`singlefile`
below.

Container Structure
^^^^^^^^^^^^^^^^^^^
The internal structure of all HDF5 files generated by ACME is kept as simple
as possible: the aggregate main file is partitioned into ``n_inputs`` groups
(`'comp_0'`, `'comp_1'`, ...) that each points to the respective payload file
actually holding the results of the corresponding computation. Within every
payload file each return value of the user-provided function `func` is saved in a
separate dataset in the file's root group. For instance, processing
the following user-provided function

.. code-block:: python

    def this_func(a, b, c):
        # ...some complicated calculations...
        return r0, r1, r2

for 50 different input triplets ``(a, b, c)`` generates one aggregate container
``this_func.h5`` and a payload of 50 HDF5 files ``this_func_0.h5``,
``this_func_1.h5``, ..., ``this_func_49.h5``. The aggregate results container
``this_func.h5`` is structured as follows:

::

    this_func.h5
        └─ comp_0
        |    └─ result_0
        |    └─ result_1
        |    └─ result_2
        └─ comp_1
        |    └─ result_0
        |    └─ result_1
        |    └─ result_2
        └─ comp_2
        |    └─ result_0
        |    └─ result_1
        |    └─ result_2
        .
        .
        .
        └─ comp_49
             └─ result_0
             └─ result_1
             └─ result_2

Each payload file ``this_func_0.h5``, ``this_func_1.h5``, ..., ``this_func_49.h5``
contains three datasets `"result_0"` (holding ``r0``), `"result_1"` (holding ``r1``)
and `"result_2"` (holding ``r2``) in its root group, e.g.,

::

    this_func_33.h5
        └─ result_0
        └─ result_1
        └─ result_2

User-provided functions with only a single return value correspondingly generate
payload files that only contain one dataset (`"result_0"`) in their respective
root group.

.. _singlefile:

Single vs. Multiple Result Files (``single_file``)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
By default, ACME generates a dedicated HDF5 file for every computational run
performed by :class:`~acme.ParallelMap` leveraging the the independent nature
of its processing tasks ("embarassingly parallel workloads"). This strategy
has the substantial advantage, that parallel workers are similarly independent
when writing results to disk: every worker generates a dedicated payload file
corresponding to the computational run it is currently processing. This lack
of shared resource use means saving does not require any synchronization:
no worker has to wait for another worker to finish its write process and releasing
a file-lock. Consequently, even tasks with perfectly distributed workloads
(all computational runs finish at the same time) can jointly save their results
without any wait time.

However, for some applications the creation of ``n_inputs`` payload files
might deteriorate performance. Depending on the underlying filesystem
generating numerous very small HDF containers may significantly slow down
I/O throughput. To remedy these problems, :class:`~acme.ParallelMap` offers
the option to write results of computational runs not separately but together
in a joint output file by setting ``single_file`` to ``True``. Consider the
function

.. code-block:: python

    def randout(x, y=3):
        if x > 0:
            return x / y
        else:
            return x * y

Suppose ``randout`` needs to be evaluated for 5000 values of `x` randomly
sampled from a standard normal distribution. To avoid the creation of 5000
payload files, use the ``single_file`` keyword in the invocation of
:class:`~acme.ParallelMap`

.. code-block:: python

    import numpy as np

    N = 5000
    rng = np.random.default_rng()
    x = rng.normal(size=N)
    with ParallelMap(randout, x, n_workers=10, single_file=True) as pmap:
        results = pmap.compute()

Note that the output does not mention the creation of a payload directory and
``results`` is a single-element list that only contains ``pmap.results_container``:

.. code-block:: python

    >>> results
    ['/my/current/workdir/ACME_20221007-100302-976973/randout.h5']
    >>> pmap.results_container
    '/my/current/workdir/ACME_20221007-100302-976973/randout.h5'

While the output of ``randout`` is small (a scalar), its execution time
for random independent input values is identical within measurement accuracy.
Thus, on a filesystem optimized for parallel I/O, running the above example
with ``single_file = False`` (default) is most likely significantly faster
since parallel workers do not have to wait for their turn to access the single
results container.

Customized Stacking of Results (``result_shape`` and ``result_dtype``)
----------------------------------------------------------------------
Most scientific data-processing functions do not return random unstructured
objects but numerical data arranged in arrays. ACME offers options to slot
incoming data into pre-defined (multi-dimensional) arrays for easier access.
Consider the function

.. code-block:: python

    import numpy as np

    def matconstruct(a, k):
        rng = np.random.default_rng(seed=k)
        i = rng.integers(low=0, high=a.shape[0], size=1)[0]
        arr = np.delete(np.corrcoef(a), i, axis=1)
        return arr

Calling ``matconstruct`` returns a 2d-array ``arr`` of shape ``(M, N)``.
Suppose, ``K = 200`` of these arrays have to be arranged in a tensor of
shape ``(K, M, N)``. Instead of letting ACME create ``K`` HDF5 groups for
each call of ``matconstruct`` which then have to be accessed post-hoc to
create the desired array, the keyword ``result_shape`` can be used to tell
:class:`~acme.ParallelMap` to slot results into a pre-allocated dataset.

.. code-block:: python

    import numpy as np

    M = 10
    N = M -1
    K = 200
    a = np.random.default_rng().random((M, 2*M))
    with ParallelMap(matconstruct, a, range(K), n_workers=50, result_shape=(None, M, N)) as pmap:
        results = pmap.compute()

Specifying ``result_shape`` impacts the container structure generated by ACME:
the results of each computational run do not need to be stored in dedicated
HDF5 groups (`'comp_0'`, `'comp_1'`, ...) but are slotted into the generated
virtual dataset. Thus, the aggregate results container only contains
a single dataset (`"result_0"`). However, if ``matconstruct`` returns multiple
quantities...

.. code-block:: python

    >>> import h5py
    >>> h5f = h5py.File(pmap.results_container, "r")
    >>> h5f.keys()

.. note::

    By default, ACME uses virtual HDF5 datasets (LINK) slotting results of
    concurrent computational runs. The real datasets in the generated payload
    files are mapped together into a single virtual dataset via the a-priori
    definition of a so-called virtual layout. The virtual data-set can be sliced,
    viewed and loaded like a regular HDF5 dataset with the virtual layout acting
    as interface layer for fetching the requested data from the associated payload
    file(s). This strategy provides a simple single-dataset interface to access
    results while maintaining the benefit of independent file access of parallel
    workers. Note that ACME can also create a single regular dataset in a single
    results container by combining ``result_shape`` with ``single_file = True``
    which comes with all benefits and downsides discussed in :ref:`singlefile`.

By default, ACME assumes the virtual dataset to contain 64-bit floating point
numbers. A different numerical datatype can be specified via the `result_dtype`
keyword:

BLAH

Note that using lower-precision numerical data-types may substantially reduce
the disk-space footprint of generated containers:

BLAH

Explain...

taskId and n_inputs:

.. _taskIDex:

Placeholder Title
-----------------
This is sometimes useful for routines that randomize in- and/or outputs, e.g.,

def randfunc(x, taskID=None):
    rng = np.random.default_rng(taskID)
    return x * rng.random()

Then the following call evaluates ``randfunc`` 5000 times using the same
input ``x = np.pi``

x = np.pi
with ParallelMap(randfunc, x, n_inputs=5000, write_worker_results=False) as pmap:
    results = pmap.compute()

TODO:
+ dashboard? + screenshot?
